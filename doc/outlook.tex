\chapter{Future Work / Outlook}
In this work, a new method for robots to quickly find objects in a dynamic environment was created. It brings together all the pieces necessary to make such a method work. However, additional functionality came up during the development of this system, which could not be realized because it was not part of the original design. Some of the existing techniques could be expanded to improve the performance and stability even further. For example, the perception module already provides some extra information that just needs to be utilized, namely the camera poses calculated alongside the object poses they enabled to recognize.

\section*{Memorising and using camera poses where objects were visible}
If an object is successfully recognized, the perception can calculate the current camera pose. It is established that this camera pose enabled the detection of an object, which could be used as rough information about where to go if such an object is needed. Currently, the perception module already publishes these poses, but they are not used any further in this work. Because each an object can be seen from many different locations and angles, an application that memorizes potentially useful camera poses would need to join them somehow, otherwise it would get cluttered quickly.

\section*{Collision detection to limit the camera field of view}
The system created in this work cleans up disappeared objects, by using the camera field of view to check if an object should currently be visible (see section~\ref{sec:impl-memo}). Currently it does not take into account that an object might be located inside the field of view, but obscured and therefore not visible. In this case, the object would be removed by mistake. As a solution, the depth image could be utilized to detect obstructions and limit the far plane of the field of view (instead of using the full range of the laser sensor).

\section*{Scaling down samples images to match an estimated object size}
Section~\ref{sec:impl-db-limits} details how the system limits the size of sample images by the camera resolution, because additional information from larger samples could never appear on camera and is therefore useless. Similarly, the estimated size of an object on camera could be used to limit the size of the sample image: As described in section~\ref{sec:impl-mipmap}, scaled down images are used for a first estimation of an objects position, so full sized images only need to be processed if an object actually appears on camera. Instead of using full sized images in the second run, samples could be scaled down to match the size of this first estimation. Multiple sample image sizes could be generated beforehand and selected according to the estimated size, so SIFT features would not have to be computed again each time an object is detected.
